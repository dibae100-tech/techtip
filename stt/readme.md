# Whisper STT Server

í•œêµ­ì–´ ìŒì„± ì¸ì‹(STT) ì„œë²„ - Faster-Whisper ê¸°ë°˜

## ğŸ¯ Features

- **ê³ í’ˆì§ˆ í•œêµ­ì–´ STT**: Whisper large-v3 ëª¨ë¸ ì‚¬ìš©
- **ë¹ ë¥¸ ì‘ë‹µ**: GPU ê°€ì†ìœ¼ë¡œ ~1ì´ˆ ë‚´ ì¸ì‹
- **REST API**: ê°„ë‹¨í•œ HTTP POST ìš”ì²­ìœ¼ë¡œ ìŒì„± ì¸ì‹
- **Buffer ì§€ì›**: Node-RED, ESP32 ë“±ì—ì„œ ì§ì ‘ ì „ì†¡ ê°€ëŠ¥

## ğŸ“‹ Requirements

- Ubuntu 22.04
- NVIDIA GPU (12GB+ VRAM ê¶Œì¥)
- CUDA 12.1+
- Python 3.12

## ğŸš€ Quick Start

### 1. Clone

```bash
git clone https://github.com/[username]/whisper-stt-server.git
cd whisper-stt-server
```

### 2. Environment Setup

```bash
conda create -n voice-ai python=3.12 -y
conda activate voice-ai
```

### 3. Install Dependencies

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
pip install faster-whisper fastapi uvicorn
```

### 4. Run Server

```bash
python stt_server.py
```

### 5. Test

```bash
curl -X POST "http://localhost:42424/stt" \
  --data-binary @test.wav \
  -H "Content-Type: audio/wav"
```

## ğŸ“¡ API

### Health Check

```
GET /health
```

Response:
```json
{"status": "ok"}
```

### Speech-to-Text

```
POST /stt
Content-Type: audio/wav
Body: [WAV íŒŒì¼ ë°”ì´ë„ˆë¦¬]
```

Response:
```json
{
  "text": "ì¸ì‹ëœ í…ìŠ¤íŠ¸",
  "time": 0.82
}
```

### Example (Python)

```python
import requests

with open('audio.wav', 'rb') as f:
    audio_bytes = f.read()

r = requests.post('http://localhost:42424/stt', data=audio_bytes)
print(r.json())
# {"text": "ì•ˆë…•í•˜ì„¸ìš”", "time": 0.82}
```

### Example (Node-RED)

Function ë…¸ë“œ:
```javascript
msg.payload = Buffer.from(msg.payload);
msg.headers = {
    "Content-Type": "audio/wav"
};
return msg;
```

HTTP request ë…¸ë“œ:
- Method: POST
- URL: \`http://[ì„œë²„IP]:42424/stt\`
- Return: a parsed JSON object

## ğŸ“ Project Structure

```
whisper-stt-server/
â”œâ”€â”€ README.md
â”œâ”€â”€ stt_server.py      # Main server
â”œâ”€â”€ test_stt.py        # Test script
â””â”€â”€ requirements.txt
```

## ğŸ”§ Server Code

```python
# stt_server.py
import time
from fastapi import FastAPI, Request
from faster_whisper import WhisperModel
import uvicorn

app = FastAPI()

print("Whisper ëª¨ë¸ ë¡œë”© ì¤‘...")
model = WhisperModel("large-v3", device="cuda", compute_type="float16")
print("STT ì„œë²„ ì¤€ë¹„ ì™„ë£Œ!")

@app.get("/health")
def health():
    return {"status": "ok"}

@app.post("/stt")
async def stt(request: Request):
    start = time.time()
    
    audio_bytes = await request.body()
    with open("/tmp/stt_input.wav", "wb") as f:
        f.write(audio_bytes)
    
    segments, info = model.transcribe("/tmp/stt_input.wav", language="ko")
    text = "".join([seg.text for seg in segments])
    
    return {"text": text, "time": round(time.time() - start, 2)}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=42424)
```

## ğŸ”„ Systemd Service

ìë™ ì‹œì‘ ë“±ë¡:

```bash
sudo cat << 'EOF' > /etc/systemd/system/stt-server.service
[Unit]
Description=Whisper STT Server
After=network.target

[Service]
Type=simple
User=ai
WorkingDirectory=/home/ai/voice-ai
ExecStart=/home/ai/miniconda3/envs/voice-ai/bin/python /home/ai/voice-ai/stt_server.py
Restart=always
RestartSec=10
Environment="PATH=/home/ai/miniconda3/envs/voice-ai/bin"

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl daemon-reload
sudo systemctl enable stt-server
sudo systemctl start stt-server
```

ì„œë¹„ìŠ¤ ê´€ë¦¬:
```bash
# ìƒíƒœ í™•ì¸
sudo systemctl status stt-server

# ë¡œê·¸ ë³´ê¸°
sudo journalctl -u stt-server -f

# ì¬ì‹œì‘
sudo systemctl restart stt-server

# ì¤‘ì§€
sudo systemctl stop stt-server
```

## âš¡ Performance

| GPU | ì‘ë‹µ ì‹œê°„ | ëª¨ë¸ |
|-----|----------|------|
| RTX 5060 Ti 16GB | ~0.8ì´ˆ | large-v3 |
| RTX 3060 12GB | ~1.5ì´ˆ | large-v3 |

## ğŸ¤ Supported Audio Format

- Format: **WAV**
- Sample Rate: 16kHz ê¶Œì¥
- Channels: Mono
- Bit Depth: 16-bit

## ğŸ“š References

- [Faster-Whisper GitHub](https://github.com/SYSTRAN/faster-whisper)
- [OpenAI Whisper](https://github.com/openai/whisper)

## ğŸ“„ License

MIT License
